{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-based Antenna Array Clustering\n",
    "\n",
    "This notebook implements Graph Neural Networks for clustering irregular antenna arrays\n",
    "using unsupervised learning with MinCut optimization.\n",
    "\n",
    "**Architecture overview:**\n",
    "1. Graph Construction: Convert antenna positions to k-NN graph\n",
    "2. GNN Layers: GAT/GCN for learning node embeddings\n",
    "3. Clustering Head: Soft assignment via softmax\n",
    "4. Loss: MinCut + Orthogonality (no labels needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration dataclasses for GNN-based antenna clustering.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional, Tuple, Union, List, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    \"\"\"\n",
    "    GNN architecture configuration.\n",
    "\n",
    "    Attributes:\n",
    "        in_dim: Input feature dimension (2 for x,y positions)\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        num_clusters: K, number of output clusters\n",
    "        num_layers: Number of GNN layers\n",
    "        heads: Number of attention heads (GAT only)\n",
    "        dropout: Dropout probability for regularization\n",
    "        layer_type: Type of GNN layer ('gat' or 'gcn')\n",
    "        use_edge_features: Whether to incorporate edge features (distance, coupling)\n",
    "    \"\"\"\n",
    "    in_dim: int = 2\n",
    "    hidden_dim: int = 64\n",
    "    num_clusters: int = 4\n",
    "    num_layers: int = 3\n",
    "    heads: int = 4\n",
    "    dropout: float = 0.1\n",
    "    layer_type: Literal[\"gat\", \"gcn\"] = \"gat\"\n",
    "    use_edge_features: bool = False\n",
    "    edge_dim: int = 1  # Dimension of edge features if used\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"\n",
    "    Graph construction configuration.\n",
    "\n",
    "    Attributes:\n",
    "        k_neighbors: Number of neighbors for k-NN graph\n",
    "        connection_type: Strategy for edge creation ('knn', 'radius', 'coupling')\n",
    "        radius: Connection radius for radius-based graphs (in normalized units)\n",
    "        coupling_threshold: Threshold for mutual coupling-based edges\n",
    "        add_self_loops: Whether to add self-loops to the adjacency matrix\n",
    "    \"\"\"\n",
    "    k_neighbors: int = 8\n",
    "    connection_type: Literal[\"knn\", \"radius\", \"coupling\"] = \"knn\"\n",
    "    radius: float = 0.5\n",
    "    coupling_threshold: float = 0.1\n",
    "    add_self_loops: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Training hyperparameters.\n",
    "\n",
    "    Attributes:\n",
    "        epochs: Number of training iterations\n",
    "        lr: Learning rate for Adam optimizer\n",
    "        weight_decay: L2 regularization coefficient\n",
    "        lambda_ortho: Weight for orthogonality loss\n",
    "        lambda_entropy: Weight for entropy regularization\n",
    "        device: Compute device ('cuda' or 'cpu')\n",
    "        verbose: Print training progress every N epochs (0 = silent)\n",
    "    \"\"\"\n",
    "    epochs: int = 500\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 5e-4\n",
    "    lambda_ortho: float = 1.0\n",
    "    lambda_entropy: float = 0.0\n",
    "    device: str = \"auto\"  # 'auto', 'cuda', or 'cpu'\n",
    "    verbose: int = 50  # Print every N epochs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClusteringConfig:\n",
    "    \"\"\"\n",
    "    Complete configuration combining all sub-configs.\n",
    "    \"\"\"\n",
    "    gnn: GNNConfig = field(default_factory=GNNConfig)\n",
    "    graph: GraphConfig = field(default_factory=GraphConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for GNN-based antenna clustering.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def normalize_positions(\n",
    "    positions: torch.Tensor,\n",
    "    method: str = \"standard\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize antenna positions for stable training.\n",
    "\n",
    "    Args:\n",
    "        positions: (N, 2) tensor of (x, y) coordinates\n",
    "        method: Normalization method\n",
    "            - \"standard\": Zero mean, unit variance (z-score)\n",
    "            - \"minmax\": Scale to [0, 1] range\n",
    "\n",
    "    Returns:\n",
    "        Normalized positions (N, 2)\n",
    "    \"\"\"\n",
    "    if method == \"standard\":\n",
    "        mean = positions.mean(dim=0, keepdim=True)\n",
    "        std = positions.std(dim=0, keepdim=True)\n",
    "        return (positions - mean) / (std + 1e-8)\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        min_val = positions.min(dim=0, keepdim=True).values\n",
    "        max_val = positions.max(dim=0, keepdim=True).values\n",
    "        return (positions - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "\n",
    "\n",
    "def get_hard_assignments(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert soft cluster probabilities to hard assignments.\n",
    "\n",
    "    c_i = argmax_k z_ik\n",
    "\n",
    "    Args:\n",
    "        z: (N, K) soft assignment matrix\n",
    "\n",
    "    Returns:\n",
    "        c: (N,) hard cluster labels in {0, ..., K-1}\n",
    "    \"\"\"\n",
    "    return z.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def get_device(device_str: str = \"auto\") -> torch.device:\n",
    "    \"\"\"\n",
    "    Get torch device from string specification.\n",
    "\n",
    "    Args:\n",
    "        device_str: \"auto\", \"cuda\", or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        torch.device object\n",
    "    \"\"\"\n",
    "    if device_str == \"auto\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return torch.device(device_str)\n",
    "\n",
    "\n",
    "def cluster_sizes(assignments: Union[torch.Tensor, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Count elements in each cluster.\n",
    "\n",
    "    Args:\n",
    "        assignments: (N,) cluster labels\n",
    "\n",
    "    Returns:\n",
    "        sizes: (K,) count per cluster\n",
    "    \"\"\"\n",
    "    if isinstance(assignments, torch.Tensor):\n",
    "        assignments = assignments.cpu().numpy()\n",
    "    return np.bincount(assignments)\n",
    "\n",
    "\n",
    "def cluster_to_list(\n",
    "    assignments: Union[torch.Tensor, np.ndarray],\n",
    "    num_clusters: Optional[int] = None\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert flat assignments to list of index arrays per cluster.\n",
    "\n",
    "    Useful for interfacing with antenna_physics.py which expects\n",
    "    List[np.ndarray] format for clusters.\n",
    "\n",
    "    Args:\n",
    "        assignments: (N,) cluster labels\n",
    "        num_clusters: K (inferred from data if None)\n",
    "\n",
    "    Returns:\n",
    "        List of K arrays, each containing indices of elements in that cluster\n",
    "    \"\"\"\n",
    "    if isinstance(assignments, torch.Tensor):\n",
    "        assignments = assignments.cpu().numpy()\n",
    "\n",
    "    if num_clusters is None:\n",
    "        num_clusters = assignments.max() + 1\n",
    "\n",
    "    clusters = []\n",
    "    for k in range(num_clusters):\n",
    "        indices = np.where(assignments == k)[0]\n",
    "        clusters.append(indices)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def assignments_to_antenna_format(\n",
    "    assignments: Union[torch.Tensor, np.ndarray],\n",
    "    grid_shape: tuple = (16, 16)\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert flat cluster assignments to antenna array format.\n",
    "\n",
    "    Compatible with AntennaArray.index_to_position_cluster() method.\n",
    "\n",
    "    Args:\n",
    "        assignments: (N,) cluster labels for flattened array\n",
    "        grid_shape: (Nz, Ny) shape of the antenna grid\n",
    "\n",
    "    Returns:\n",
    "        List of K arrays with shape (L_k, 2) containing [col, row] indices\n",
    "    \"\"\"\n",
    "    if isinstance(assignments, torch.Tensor):\n",
    "        assignments = assignments.cpu().numpy()\n",
    "\n",
    "    Nz, Ny = grid_shape\n",
    "    num_clusters = assignments.max() + 1\n",
    "\n",
    "    clusters = []\n",
    "    for k in range(num_clusters):\n",
    "        flat_indices = np.where(assignments == k)[0]\n",
    "\n",
    "        # Convert flat index to 2D grid indices\n",
    "        # Assuming row-major (C) ordering: flat_idx = row * Ny + col\n",
    "        rows = flat_indices // Ny\n",
    "        cols = flat_indices % Ny\n",
    "\n",
    "        # Format as [col, row] to match antenna_physics convention\n",
    "        cluster_coords = np.stack([cols, rows], axis=1)\n",
    "        clusters.append(cluster_coords)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def compute_clustering_metrics(\n",
    "    assignments: np.ndarray,\n",
    "    positions: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute basic clustering quality metrics.\n",
    "\n",
    "    Args:\n",
    "        assignments: (N,) cluster labels\n",
    "        positions: (N, 2) antenna positions\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with metrics:\n",
    "            - num_clusters: Actual number of non-empty clusters\n",
    "            - cluster_sizes: Elements per cluster\n",
    "            - size_variance: Variance in cluster sizes\n",
    "            - mean_intra_distance: Average within-cluster distance\n",
    "    \"\"\"\n",
    "    # TODO: Implement clustering metrics\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph construction utilities for antenna array clustering.\n",
    "\n",
    "Converts antenna positions into graph structures (nodes, edges, adjacency matrix).\n",
    "Supports multiple edge creation strategies: k-NN, radius-based, mutual coupling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds graph representations from antenna positions.\n",
    "\n",
    "    The antenna array is represented as:\n",
    "        - Nodes: Individual antenna elements\n",
    "        - Edges: Connections based on proximity or coupling\n",
    "        - Node features: Positions (and optionally other attributes)\n",
    "        - Edge features: Distance, mutual coupling magnitude/phase\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[GraphConfig] = None):\n",
    "        self.config = config or GraphConfig()\n",
    "\n",
    "    def build_knn_edges(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        k: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build edge index using k-nearest neighbors.\n",
    "\n",
    "        Args:\n",
    "            positions: (N, 2) tensor of antenna positions\n",
    "            k: Number of neighbors (uses config default if None)\n",
    "\n",
    "        Returns:\n",
    "            edge_index: (2, E) tensor of edge indices in COO format\n",
    "        \"\"\"\n",
    "        # TODO: Implement k-NN edge construction\n",
    "        # Use torch_geometric.nn.knn_graph or scipy.spatial.cKDTree\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_radius_edges(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        radius: Optional[float] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build edge index using radius-based connectivity (epsilon-ball).\n",
    "\n",
    "        Connects all nodes within distance `radius` of each other.\n",
    "\n",
    "        Args:\n",
    "            positions: (N, 2) tensor of antenna positions\n",
    "            radius: Connection radius (uses config default if None)\n",
    "\n",
    "        Returns:\n",
    "            edge_index: (2, E) tensor of edge indices\n",
    "        \"\"\"\n",
    "        # TODO: Implement radius-based edge construction\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_coupling_edges(\n",
    "        self,\n",
    "        coupling_matrix: torch.Tensor,\n",
    "        threshold: Optional[float] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build edge index based on mutual coupling magnitude.\n",
    "\n",
    "        Connects antennas with coupling above threshold.\n",
    "\n",
    "        Args:\n",
    "            coupling_matrix: (N, N) complex mutual coupling matrix M\n",
    "            threshold: Minimum |M_ij| to create edge\n",
    "\n",
    "        Returns:\n",
    "            edge_index: (2, E) tensor of edge indices\n",
    "        \"\"\"\n",
    "        # TODO: Implement coupling-based edge construction\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_adjacency_matrix(\n",
    "        self,\n",
    "        edge_index: torch.Tensor,\n",
    "        num_nodes: int,\n",
    "        symmetric: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert edge index to dense adjacency matrix A.\n",
    "\n",
    "        Args:\n",
    "            edge_index: (2, E) COO format edges\n",
    "            num_nodes: N, number of nodes\n",
    "            symmetric: Symmetrize the adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            adj: (N, N) adjacency matrix\n",
    "        \"\"\"\n",
    "        # TODO: Implement adjacency matrix construction\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_degree_matrix(self, adj: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute degree matrix D from adjacency matrix.\n",
    "\n",
    "        D_ii = sum_j A_ij (number of neighbors)\n",
    "\n",
    "        Args:\n",
    "            adj: (N, N) adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            deg: (N, N) diagonal degree matrix\n",
    "        \"\"\"\n",
    "        # TODO: Implement degree matrix computation\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_edge_features(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        coupling_matrix: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute physics-informed edge features.\n",
    "\n",
    "        Features per edge (i, j):\n",
    "            - Euclidean distance d_ij\n",
    "            - Mutual coupling magnitude |M_ij| (if provided)\n",
    "            - Mutual coupling phase angle(M_ij) (if provided)\n",
    "\n",
    "        Args:\n",
    "            positions: (N, 2) node positions\n",
    "            edge_index: (2, E) edges\n",
    "            coupling_matrix: (N, N) optional mutual coupling matrix\n",
    "\n",
    "        Returns:\n",
    "            edge_attr: (E, F) edge feature matrix\n",
    "        \"\"\"\n",
    "        # TODO: Implement edge feature computation\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_graph(\n",
    "        self,\n",
    "        positions: Union[torch.Tensor, np.ndarray],\n",
    "        coupling_matrix: Optional[Union[torch.Tensor, np.ndarray]] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Build complete graph representation from antenna positions.\n",
    "\n",
    "        This is the main entry point for graph construction.\n",
    "\n",
    "        Args:\n",
    "            positions: (N, 2) antenna positions\n",
    "            coupling_matrix: (N, N) optional mutual coupling matrix\n",
    "\n",
    "        Returns:\n",
    "            Tuple of:\n",
    "                - edge_index: (2, E) edge indices\n",
    "                - adj: (N, N) adjacency matrix\n",
    "                - deg: (N, N) degree matrix\n",
    "                - edge_attr: (E, F) edge features (or None)\n",
    "        \"\"\"\n",
    "        # TODO: Implement complete graph building pipeline\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def normalized_adjacency(adj: torch.Tensor, deg: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute normalized adjacency: D^{-1/2} A D^{-1/2}\n",
    "\n",
    "    Used in GCN to prevent exploding/vanishing gradients during message passing.\n",
    "\n",
    "    Args:\n",
    "        adj: (N, N) adjacency matrix (with or without self-loops)\n",
    "        deg: (N, N) degree matrix\n",
    "\n",
    "    Returns:\n",
    "        norm_adj: (N, N) normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    # TODO: Implement symmetric normalization\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GNN layer implementations for antenna clustering.\n",
    "\n",
    "Implements message passing layers:\n",
    "    - GCN (Graph Convolutional Network): Simple aggregation with learned weights\n",
    "    - GAT (Graph Attention Network): Attention-weighted aggregation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network layer.\n",
    "\n",
    "    Implements: H^{l+1} = sigma(D^{-1/2} A D^{-1/2} H^{l} W^{l})\n",
    "\n",
    "    Each node aggregates normalized neighbor features, applies linear transform,\n",
    "    then non-linearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension per node\n",
    "            out_features: Output dimension per node\n",
    "            bias: Include learnable bias term\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Learnable weight matrix W\n",
    "        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initialize weights using Xavier/Glorot initialization.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        adj_norm: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (N, in_features) node features\n",
    "            adj_norm: (N, N) normalized adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            out: (N, out_features) updated node features\n",
    "        \"\"\"\n",
    "        # TODO: Implement GCN forward pass\n",
    "        # 1. Linear transform: XW\n",
    "        # 2. Neighborhood aggregation: A_norm @ (XW)\n",
    "        # 3. Add bias\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network layer.\n",
    "\n",
    "    Learns attention weights to focus on relevant neighbors during aggregation.\n",
    "    Supports multi-head attention for richer representations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        heads: int = 1,\n",
    "        concat: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        negative_slope: float = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension per node\n",
    "            out_features: Output dimension per head\n",
    "            heads: Number of attention heads\n",
    "            concat: Concatenate heads (True) or average (False)\n",
    "            dropout: Dropout on attention weights\n",
    "            negative_slope: LeakyReLU negative slope\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "        # Linear transform for node features: W\n",
    "        self.W = nn.Parameter(torch.empty(heads, in_features, out_features))\n",
    "\n",
    "        # Attention mechanism parameters: a = [a_l || a_r]\n",
    "        self.a_l = nn.Parameter(torch.empty(heads, out_features, 1))\n",
    "        self.a_r = nn.Parameter(torch.empty(heads, out_features, 1))\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initialize using Xavier/Glorot.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.a_l)\n",
    "        nn.init.xavier_uniform_(self.a_r)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x: (N, in_features) node features\n",
    "            edge_index: (2, E) edge indices in COO format\n",
    "            edge_attr: (E, edge_dim) optional edge features\n",
    "\n",
    "        Returns:\n",
    "            out: (N, heads * out_features) if concat else (N, out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement GAT forward pass\n",
    "        # 1. Linear transform: Wh_i for all nodes\n",
    "        # 2. Compute attention coefficients: e_ij = LeakyReLU(a^T [Wh_i || Wh_j])\n",
    "        # 3. Normalize with softmax over neighbors: alpha_ij\n",
    "        # 4. Aggregate: h'_i = sum_j alpha_ij * Wh_j\n",
    "        # 5. Concatenate or average heads\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_attention(\n",
    "        self,\n",
    "        h_l: torch.Tensor,\n",
    "        h_r: torch.Tensor,\n",
    "        edge_index: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute attention coefficients for each edge.\n",
    "\n",
    "        Args:\n",
    "            h_l: (N, heads, out_features) source node representations\n",
    "            h_r: (N, heads, out_features) target node representations\n",
    "            edge_index: (2, E) edges\n",
    "\n",
    "        Returns:\n",
    "            alpha: (E, heads) attention weights\n",
    "        \"\"\"\n",
    "        # TODO: Implement attention computation\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class EdgeConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Edge-conditioned convolution layer.\n",
    "\n",
    "    Incorporates edge features (distance, coupling) into message passing.\n",
    "    Useful for physics-informed learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        edge_features: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Node feature dimension\n",
    "            out_features: Output dimension\n",
    "            edge_features: Edge feature dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.edge_features = edge_features\n",
    "\n",
    "        # MLP for combining node and edge features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * in_features + edge_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with edge features.\n",
    "\n",
    "        Args:\n",
    "            x: (N, in_features) node features\n",
    "            edge_index: (2, E) edges\n",
    "            edge_attr: (E, edge_features) edge features\n",
    "\n",
    "        Returns:\n",
    "            out: (N, out_features) updated features\n",
    "        \"\"\"\n",
    "        # TODO: Implement edge-conditioned message passing\n",
    "        # h'_i = sum_j MLP([h_i || h_j || e_ij])\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss functions for unsupervised GNN clustering.\n",
    "\n",
    "No labels are needed - clustering quality is measured by graph structure:\n",
    "    - MinCut: Minimize edges between clusters (maximize within-cluster connectivity)\n",
    "    - Orthogonality: Ensure balanced, non-overlapping clusters\n",
    "    - Entropy: Encourage confident (non-uniform) assignments\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mincut_loss(\n",
    "    z: torch.Tensor,\n",
    "    adj: torch.Tensor,\n",
    "    deg: torch.Tensor,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalized MinCut loss.\n",
    "\n",
    "    L_cut = -Tr(Z^T A Z) / Tr(Z^T D Z)\n",
    "\n",
    "    Minimizing this loss maximizes within-cluster edges (good clustering\n",
    "    keeps connected nodes together).\n",
    "\n",
    "    Args:\n",
    "        z: (N, K) soft cluster assignment matrix\n",
    "        adj: (N, N) adjacency matrix\n",
    "        deg: (N, N) degree matrix (diagonal)\n",
    "        eps: Small constant for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar MinCut loss (negative, to be minimized)\n",
    "    \"\"\"\n",
    "    # TODO: Implement MinCut loss\n",
    "    # Z^T A Z measures within-cluster edges\n",
    "    # Z^T D Z normalizes by cluster sizes\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def orthogonality_loss(\n",
    "    z: torch.Tensor,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Orthogonality regularization loss.\n",
    "\n",
    "    L_ortho = || Z^T Z / N  -  I_K / K ||_F^2\n",
    "\n",
    "    Prevents trivial solution where all nodes go to one cluster.\n",
    "    Encourages balanced cluster sizes.\n",
    "\n",
    "    Args:\n",
    "        z: (N, K) soft cluster assignment matrix\n",
    "        eps: Small constant for stability\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar orthogonality loss\n",
    "    \"\"\"\n",
    "    # TODO: Implement orthogonality loss\n",
    "    # Z^T Z should be close to (N/K) * I_K for balanced clusters\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def entropy_loss(\n",
    "    z: torch.Tensor,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Entropy regularization loss.\n",
    "\n",
    "    L_entropy = -(1/N) * sum_i sum_k z_ik * log(z_ik)\n",
    "\n",
    "    Low entropy means confident predictions (z_ik close to 0 or 1).\n",
    "    Can be used to encourage sharper cluster assignments.\n",
    "\n",
    "    Args:\n",
    "        z: (N, K) soft cluster assignment matrix\n",
    "        eps: Small constant to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar mean entropy (negative for confident assignments)\n",
    "    \"\"\"\n",
    "    # TODO: Implement entropy loss\n",
    "    # Shannon entropy averaged over all nodes\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def cluster_size_loss(\n",
    "    z: torch.Tensor,\n",
    "    target_size: Optional[int] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Cluster size regularization.\n",
    "\n",
    "    Penalizes deviation from uniform cluster sizes.\n",
    "    Optional: can target specific cluster sizes.\n",
    "\n",
    "    Args:\n",
    "        z: (N, K) soft cluster assignment matrix\n",
    "        target_size: Target elements per cluster (default: N/K)\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar size variance loss\n",
    "    \"\"\"\n",
    "    # TODO: Implement cluster size regularization\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def total_loss(\n",
    "    z: torch.Tensor,\n",
    "    adj: torch.Tensor,\n",
    "    deg: torch.Tensor,\n",
    "    lambda_ortho: float = 1.0,\n",
    "    lambda_entropy: float = 0.0,\n",
    "    eps: float = 1e-8\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combined loss function.\n",
    "\n",
    "    L_total = L_cut + lambda_ortho * L_ortho + lambda_entropy * L_entropy\n",
    "\n",
    "    Args:\n",
    "        z: (N, K) soft cluster assignments\n",
    "        adj: (N, N) adjacency matrix\n",
    "        deg: (N, N) degree matrix\n",
    "        lambda_ortho: Weight for orthogonality term\n",
    "        lambda_entropy: Weight for entropy term (optional)\n",
    "        eps: Numerical stability constant\n",
    "\n",
    "    Returns:\n",
    "        loss: Total scalar loss\n",
    "    \"\"\"\n",
    "    loss = mincut_loss(z, adj, deg, eps)\n",
    "    loss = loss + lambda_ortho * orthogonality_loss(z, eps)\n",
    "\n",
    "    if lambda_entropy > 0:\n",
    "        loss = loss + lambda_entropy * entropy_loss(z, eps)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ClusteringLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Module wrapper for clustering loss computation.\n",
    "\n",
    "    Convenient for use in training loops with configurable weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lambda_ortho: float = 1.0,\n",
    "        lambda_entropy: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lambda_ortho: Weight for orthogonality loss\n",
    "            lambda_entropy: Weight for entropy loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lambda_ortho = lambda_ortho\n",
    "        self.lambda_entropy = lambda_entropy\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: torch.Tensor,\n",
    "        adj: torch.Tensor,\n",
    "        deg: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute total loss.\n",
    "\n",
    "        Args:\n",
    "            z: Soft cluster assignments\n",
    "            adj: Adjacency matrix\n",
    "            deg: Degree matrix\n",
    "\n",
    "        Returns:\n",
    "            Total loss scalar\n",
    "        \"\"\"\n",
    "        return total_loss(\n",
    "            z, adj, deg,\n",
    "            self.lambda_ortho,\n",
    "            self.lambda_entropy\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main GNN model for antenna array clustering.\n",
    "\n",
    "Architecture:\n",
    "    Input(N, 2) -> GAT/GCN layers -> Embeddings(N, d) -> Linear+Softmax -> Z(N, K)\n",
    "\n",
    "The model outputs soft cluster assignments Z where Z_ik = P(node i in cluster k).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AntennaClusteringGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN model for unsupervised clustering of irregular antenna arrays.\n",
    "\n",
    "    Pipeline:\n",
    "        1. Stack of GAT/GCN layers to learn node embeddings\n",
    "        2. Final linear layer to map embeddings to K cluster logits\n",
    "        3. Softmax to get soft cluster probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[GNNConfig] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: GNN architecture configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config or GNNConfig()\n",
    "\n",
    "        self._build_layers()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"Construct GNN layers based on configuration.\"\"\"\n",
    "        cfg = self.config\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        # Input dimension for first layer\n",
    "        in_dim = cfg.in_dim\n",
    "\n",
    "        # Build GNN layers\n",
    "        for i in range(cfg.num_layers):\n",
    "            # Last layer: single head, no concat\n",
    "            is_last = (i == cfg.num_layers - 1)\n",
    "\n",
    "            if cfg.layer_type == \"gat\":\n",
    "                heads = 1 if is_last else cfg.heads\n",
    "                out_dim = cfg.hidden_dim\n",
    "                layer = GATLayer(\n",
    "                    in_features=in_dim,\n",
    "                    out_features=out_dim,\n",
    "                    heads=heads,\n",
    "                    concat=not is_last,\n",
    "                    dropout=cfg.dropout\n",
    "                )\n",
    "                # Update input dim for next layer\n",
    "                in_dim = out_dim if is_last else out_dim * heads\n",
    "            else:  # gcn\n",
    "                out_dim = cfg.hidden_dim\n",
    "                layer = GCNLayer(\n",
    "                    in_features=in_dim,\n",
    "                    out_features=out_dim\n",
    "                )\n",
    "                in_dim = out_dim\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # Optional: layer normalization\n",
    "            if not is_last:\n",
    "                self.norms.append(nn.LayerNorm(in_dim))\n",
    "\n",
    "        # Final embedding dimension\n",
    "        self.embed_dim = in_dim\n",
    "\n",
    "        # Output layer: embeddings -> cluster logits\n",
    "        self.classifier = nn.Linear(self.embed_dim, cfg.num_clusters)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        adj_norm: Optional[torch.Tensor] = None,\n",
    "        edge_attr: Optional[torch.Tensor] = None,\n",
    "        return_embeddings: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (N, in_dim) input node features (positions)\n",
    "            edge_index: (2, E) edges in COO format\n",
    "            adj_norm: (N, N) normalized adjacency (for GCN)\n",
    "            edge_attr: (E, edge_dim) edge features (optional)\n",
    "            return_embeddings: Also return intermediate embeddings\n",
    "\n",
    "        Returns:\n",
    "            z: (N, K) soft cluster assignment probabilities\n",
    "            h: (N, embed_dim) node embeddings (if return_embeddings=True)\n",
    "        \"\"\"\n",
    "        h = x\n",
    "\n",
    "        # Pass through GNN layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, GATLayer):\n",
    "                h = layer(h, edge_index, edge_attr)\n",
    "            else:  # GCN\n",
    "                h = layer(h, adj_norm)\n",
    "\n",
    "            # Activation + dropout (except last layer)\n",
    "            if i < len(self.layers) - 1:\n",
    "                h = F.elu(h)\n",
    "                h = self.dropout(h)\n",
    "                if i < len(self.norms):\n",
    "                    h = self.norms[i](h)\n",
    "\n",
    "        # Final activation\n",
    "        h = F.elu(h)\n",
    "\n",
    "        # Cluster probabilities via softmax\n",
    "        logits = self.classifier(h)\n",
    "        z = F.softmax(logits, dim=-1)\n",
    "\n",
    "        if return_embeddings:\n",
    "            return z, h\n",
    "        return z, None\n",
    "\n",
    "    def get_hard_assignments(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert soft probabilities to hard cluster assignments.\n",
    "\n",
    "        c_i = argmax_k z_ik\n",
    "\n",
    "        Args:\n",
    "            z: (N, K) soft assignment probabilities\n",
    "\n",
    "        Returns:\n",
    "            c: (N,) hard cluster labels in {0, ..., K-1}\n",
    "        \"\"\"\n",
    "        return z.argmax(dim=-1)\n",
    "\n",
    "    @property\n",
    "    def num_parameters(self) -> int:\n",
    "        \"\"\"Total number of trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class AntennaClusteringGNNWithEdgeFeatures(AntennaClusteringGNN):\n",
    "    \"\"\"\n",
    "    Extended model that incorporates physics-informed edge features.\n",
    "\n",
    "    Edge features can include:\n",
    "        - Euclidean distance between antennas\n",
    "        - Mutual coupling magnitude\n",
    "        - Mutual coupling phase\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[GNNConfig] = None):\n",
    "        # Ensure edge features are enabled\n",
    "        if config is None:\n",
    "            config = GNNConfig(use_edge_features=True)\n",
    "        else:\n",
    "            config.use_edge_features = True\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"Build layers with edge feature support.\"\"\"\n",
    "        cfg = self.config\n",
    "\n",
    "        # Edge feature encoder\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(cfg.edge_dim, cfg.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.hidden_dim, cfg.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Rest of the architecture\n",
    "        super()._build_layers()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        adj_norm: Optional[torch.Tensor] = None,\n",
    "        edge_attr: Optional[torch.Tensor] = None,\n",
    "        return_embeddings: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"Forward pass with edge feature processing.\"\"\"\n",
    "        # Encode edge features\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # Standard forward pass\n",
    "        return super().forward(\n",
    "            x, edge_index, adj_norm, edge_attr, return_embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training utilities for GNN-based antenna clustering.\n",
    "\n",
    "Provides a Trainer class that handles:\n",
    "    - Model initialization\n",
    "    - Graph construction from positions\n",
    "    - Training loop with loss computation\n",
    "    - Cluster extraction\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingResult:\n",
    "    \"\"\"Container for training results.\"\"\"\n",
    "    cluster_assignments: np.ndarray  # (N,) hard cluster labels\n",
    "    soft_assignments: np.ndarray     # (N, K) cluster probabilities\n",
    "    loss_history: List[float]        # Loss per epoch\n",
    "    final_loss: float\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer for GNN-based antenna array clustering.\n",
    "\n",
    "    Usage:\n",
    "        trainer = Trainer(num_clusters=4)\n",
    "        result = trainer.fit(positions)\n",
    "        clusters = result.cluster_assignments\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_clusters: int = 4,\n",
    "        gnn_config: Optional[GNNConfig] = None,\n",
    "        graph_config: Optional[GraphConfig] = None,\n",
    "        training_config: Optional[TrainingConfig] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_clusters: Number of clusters K\n",
    "            gnn_config: GNN architecture config\n",
    "            graph_config: Graph construction config\n",
    "            training_config: Training hyperparameters\n",
    "        \"\"\"\n",
    "        self.gnn_config = gnn_config or GNNConfig(num_clusters=num_clusters)\n",
    "        self.gnn_config.num_clusters = num_clusters\n",
    "\n",
    "        self.graph_config = graph_config or GraphConfig()\n",
    "        self.training_config = training_config or TrainingConfig()\n",
    "\n",
    "        self.model: Optional[AntennaClusteringGNN] = None\n",
    "        self.graph_builder = GraphBuilder(self.graph_config)\n",
    "\n",
    "        # Cached graph data\n",
    "        self._edge_index: Optional[torch.Tensor] = None\n",
    "        self._adj: Optional[torch.Tensor] = None\n",
    "        self._deg: Optional[torch.Tensor] = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        positions: Union[np.ndarray, torch.Tensor],\n",
    "        coupling_matrix: Optional[Union[np.ndarray, torch.Tensor]] = None\n",
    "    ) -> TrainingResult:\n",
    "        \"\"\"\n",
    "        Train the GNN model on antenna positions.\n",
    "\n",
    "        Args:\n",
    "            positions: (N, 2) array of antenna (x, y) positions\n",
    "            coupling_matrix: (N, N) optional mutual coupling matrix\n",
    "\n",
    "        Returns:\n",
    "            TrainingResult with cluster assignments and training info\n",
    "        \"\"\"\n",
    "        # Setup device\n",
    "        device = get_device(self.training_config.device)\n",
    "\n",
    "        # Prepare data\n",
    "        positions = self._prepare_positions(positions, device)\n",
    "        n_nodes = positions.shape[0]\n",
    "\n",
    "        # Build graph\n",
    "        self._build_graph(positions, coupling_matrix, device)\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = AntennaClusteringGNN(self.gnn_config).to(device)\n",
    "\n",
    "        # Setup optimizer and loss\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.training_config.lr,\n",
    "            weight_decay=self.training_config.weight_decay\n",
    "        )\n",
    "        criterion = ClusteringLoss(\n",
    "            lambda_ortho=self.training_config.lambda_ortho,\n",
    "            lambda_entropy=self.training_config.lambda_entropy\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        loss_history = self._train_loop(\n",
    "            positions, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "        # Extract results\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            z, _ = self.model(positions, self._edge_index, self._adj_norm)\n",
    "            clusters = z.argmax(dim=-1).cpu().numpy()\n",
    "            soft_assignments = z.cpu().numpy()\n",
    "\n",
    "        return TrainingResult(\n",
    "            cluster_assignments=clusters,\n",
    "            soft_assignments=soft_assignments,\n",
    "            loss_history=loss_history,\n",
    "            final_loss=loss_history[-1] if loss_history else float('inf')\n",
    "        )\n",
    "\n",
    "    def _prepare_positions(\n",
    "        self,\n",
    "        positions: Union[np.ndarray, torch.Tensor],\n",
    "        device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Convert and normalize positions.\"\"\"\n",
    "        if isinstance(positions, np.ndarray):\n",
    "            positions = torch.from_numpy(positions).float()\n",
    "\n",
    "        positions = normalize_positions(positions)\n",
    "        return positions.to(device)\n",
    "\n",
    "    def _build_graph(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        coupling_matrix: Optional[Union[np.ndarray, torch.Tensor]],\n",
    "        device: torch.device\n",
    "    ):\n",
    "        \"\"\"Construct graph from positions.\"\"\"\n",
    "        # TODO: Implement using GraphBuilder\n",
    "        # For now, create placeholder tensors\n",
    "        n = positions.shape[0]\n",
    "\n",
    "        # Placeholder edge_index (will be built by GraphBuilder)\n",
    "        self._edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)\n",
    "\n",
    "        # Placeholder adjacency and degree matrices\n",
    "        self._adj = torch.zeros((n, n), device=device)\n",
    "        self._deg = torch.zeros((n, n), device=device)\n",
    "        self._adj_norm = torch.zeros((n, n), device=device)\n",
    "\n",
    "    def _train_loop(\n",
    "        self,\n",
    "        positions: torch.Tensor,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        criterion: ClusteringLoss,\n",
    "        device: torch.device\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Execute training loop.\"\"\"\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(self.training_config.epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            z, _ = self.model(positions, self._edge_index, self._adj_norm)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(z, self._adj, self._deg)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val = loss.item()\n",
    "            loss_history.append(loss_val)\n",
    "\n",
    "            # Logging\n",
    "            if self.training_config.verbose > 0:\n",
    "                if (epoch + 1) % self.training_config.verbose == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.training_config.epochs}: \"\n",
    "                          f\"Loss = {loss_val:.4f}\")\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        positions: Union[np.ndarray, torch.Tensor]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get cluster assignments for new positions (after training).\n",
    "\n",
    "        Args:\n",
    "            positions: (N, 2) antenna positions\n",
    "\n",
    "        Returns:\n",
    "            clusters: (N,) cluster labels\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        positions = self._prepare_positions(positions, device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            z, _ = self.model(positions, self._edge_index, self._adj_norm)\n",
    "            return z.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "\n",
    "def train_clustering(\n",
    "    positions: Union[np.ndarray, torch.Tensor],\n",
    "    num_clusters: int = 4,\n",
    "    k_neighbors: int = 8,\n",
    "    epochs: int = 500,\n",
    "    lr: float = 1e-3,\n",
    "    verbose: int = 50\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convenience function for quick clustering.\n",
    "\n",
    "    Args:\n",
    "        positions: (N, 2) antenna positions\n",
    "        num_clusters: Number of clusters K\n",
    "        k_neighbors: Neighbors for graph construction\n",
    "        epochs: Training iterations\n",
    "        lr: Learning rate\n",
    "        verbose: Print progress every N epochs\n",
    "\n",
    "    Returns:\n",
    "        clusters: (N,) array of cluster labels\n",
    "    \"\"\"\n",
    "    trainer = Trainer(\n",
    "        num_clusters=num_clusters,\n",
    "        graph_config=GraphConfig(k_neighbors=k_neighbors),\n",
    "        training_config=TrainingConfig(epochs=epochs, lr=lr, verbose=verbose)\n",
    "    )\n",
    "    result = trainer.fit(positions)\n",
    "    return result.cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create dummy antenna positions and run clustering\n",
    "\n",
    "# Generate a 4x4 grid of antenna positions\n",
    "# positions = np.array([[i, j] for i in range(4) for j in range(4)], dtype=np.float32)\n",
    "# print(f\"Antenna positions shape: {positions.shape}\")\n",
    "\n",
    "# Run clustering (will fail until TODOs are implemented)\n",
    "# clusters = train_clustering(positions, num_clusters=4, epochs=100, verbose=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
