\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
}

\title{GNN-Based Antenna Array Clustering\\[6pt]
\large Cell-by-Cell Explanation of \texttt{gnn.ipynb}}
\author{}
\date{}

\begin{document}
\maketitle
\tableofcontents
\newpage

% ============================================================
\section{Problem Statement}
% ============================================================

\subsection{What Is the Physical Setup?}

We have a \textbf{Uniform Rectangular Array (URA)} of $16 \times 16 = 256$ antenna elements
operating at $29.5\;\text{GHz}$ (millimeter-wave, 5G NR band).
The elements are spaced $0.5\lambda$ apart horizontally and $0.7\lambda$ apart vertically,
where $\lambda = c / f \approx 1.017\;\text{cm}$.

In a phased array, each antenna element needs its own phase-shifter and RF chain.
With 256 elements, this is expensive.
\textbf{Clustering} (also called sub-arraying) groups nearby elements so that all elements
in the same cluster share a single phase-shifter.
This drastically reduces hardware cost: instead of 256 independent RF chains, we only need
$K$ clusters (where $K$ is automatically discovered by the GNN to achieve a target
clustering factor).

\subsection{What Do We Want to Optimize?}

The goal is to partition the 256 elements into clusters such that:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Target clustering factor is achieved}: the clustering factor
          $\text{CF} = N / K$ (average elements per cluster) is close to a specified target.
          The number of clusters $K$ is not fixed \emph{a priori}---the GNN discovers it
          automatically.
    \item \textbf{Clusters are spatially contiguous}: elements in the same cluster should be
          physically close (not scattered across the array).
    \item \textbf{Radiation performance is preserved}: the far-field pattern should maintain
          high main-lobe gain, low side-lobe levels (SLL), and narrow beamwidths (HPBW).
    \item \textbf{Cost function $C_m$ is minimized}: $C_m$ measures the mean of the top-10
          worst violations of the SLL mask in the far-field pattern---ideally $C_m = 0$.
\end{enumerate}

\subsection{How Do We Optimize?}

We use a \textbf{Graph Neural Network (GNN)} trained with \emph{unsupervised} loss functions
in a \emph{physics-informed} manner: mutual coupling between antenna elements is used as
edge weights in the graph.
The 256 antennas become \emph{nodes} of a graph; edges connect physically adjacent elements
and carry mutual coupling magnitudes.
The GNN learns a soft assignment matrix $\mathbf{Z} \in \mathbb{R}^{N \times K_{\max}}$
where $Z_{ik}$ is the probability that element $i$ belongs to cluster $k$, and $K_{\max}$
is an upper bound on the number of clusters.
The GNN automatically deactivates unused clusters, discovering the optimal $K$.
Hard assignments are obtained by $\text{cluster}(i) = \arg\max_k Z_{ik}$.

During training, the model uses \textbf{Gumbel-Softmax} (instead of plain softmax) to produce
the soft assignments $\mathbf{Z}$.
Gumbel-Softmax adds stochastic noise to the logits, which breaks the symmetry of the initial
uniform assignments and ensures non-zero gradients flow through the MinCut loss.
A \textbf{temperature annealing} schedule controls the trade-off between exploration
(high temperature, soft assignments) and exploitation (low temperature, hard assignments):
the temperature starts at $T=5.0$ and is linearly annealed to $T=0.5$ over the first 70\%
of training epochs.
At inference time, standard softmax is used for deterministic assignments.

\subsection{Inputs and Outputs Summary}

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{p{3cm} p{10cm}}
\toprule
\textbf{Inputs} & 256 antenna positions on a $16\times16$ grid; spacing $d_x=0.5\lambda$,
$d_y=0.7\lambda$; frequency $29.5\;\text{GHz}$; target clustering factor $\text{CF}_{\text{target}}$;
beam steering direction $(\theta_0, \phi_0)$; SLL mask thresholds;
optionally, a set of allowed cluster sizes (e.g.\ $\{1, 2, 4\}$). \\
\midrule
\textbf{Optimization} & Physics-informed unsupervised GNN minimizing a composite loss
(coupling MinCut + clustering factor + balance + entropy + contiguity + optional allowed-sizes).
Mutual coupling used as edge weights. Gumbel-Softmax with temperature annealing.
No labelled data needed. \\
\midrule
\textbf{Outputs} & Cluster assignment vector $\mathbf{c} \in \{0,\dots,K{-}1\}^{256}$
where $K$ is automatically discovered;
far-field radiation pattern; performance metrics (gain, HPBW, SLL, $C_m$);
hardware reduction percentage $= (1 - K/N) \times 100\%$. \\
\bottomrule
\end{tabular}
\end{center}

\newpage
% ============================================================
\section{Cell-by-Cell Explanation}
% ============================================================

% ------------------------------------------------------------
\subsection{Cell 0 --- Title (Markdown)}
% ------------------------------------------------------------
A markdown header declaring the notebook title:
\emph{``GNN-based URA Clustering --- trains a Graph Neural Network to cluster a
$16\times16$ antenna array.''}
No code is executed.

% ------------------------------------------------------------
\subsection{Cell 1 --- Imports and Seeds}
% ------------------------------------------------------------

\textbf{What it does.}
Imports the required libraries and sets random seeds for reproducibility.

\begin{itemize}
    \item \texttt{numpy} --- numerical arrays and linear algebra.
    \item \texttt{torch} --- PyTorch deep learning framework (the GNN runs on PyTorch).
    \item \texttt{matplotlib} --- all plots.
    \item \texttt{sys.path.insert(0, '..')} --- adds the parent directory to Python's search
          path so that the custom \texttt{gnn} package (which lives one level up) can be imported.
    \item From the \texttt{gnn} package it imports:
    \begin{itemize}
        \item \texttt{URAConfig} --- a dataclass holding array geometry (rows, cols, spacing, frequency).
        \item \texttt{create\_ura\_graph} --- builds a graph from the antenna grid (nodes = elements,
              edges = neighbors), with mutual-coupling edge weights.
        \item \texttt{compute\_mutual\_coupling} --- computes an $N \times N$ complex coupling matrix
              using a simplified analytical model: $M_{ij} = e^{-j2\pi r_{ij}} / (r_{ij} + 0.1)$
              with edge/corner corrections.
        \item \texttt{train\_ura\_clustering} --- physics-informed training loop with target
              clustering factor, Gumbel-Softmax, and temperature annealing.
              The GNN discovers the optimal number of clusters automatically.
        \item \texttt{assignments\_to\_antenna\_format} --- converts a flat cluster-label vector
              into the list-of-coordinate-arrays format expected by the antenna physics engine.
    \end{itemize}
    \item Sets \texttt{torch.manual\_seed(42)} and \texttt{np.random.seed(42)} so that every run
          produces the same results.
\end{itemize}

\textbf{Output.}
Prints the PyTorch version and whether CUDA is available.

% ------------------------------------------------------------
\subsection{Checkpoint Utilities Cell}
% ------------------------------------------------------------

\textbf{What it does.}
Defines helper functions for saving intermediate results to JSON files in the
\texttt{intermediate\_steps/} directory:

\begin{itemize}
    \item \texttt{\_to\_serializable} / \texttt{\_convert} --- recursively convert numpy arrays
          and torch tensors to JSON-serializable Python types.
    \item \texttt{\_build\_config\_step} --- creates a config dict from a \texttt{URAConfig} object.
    \item \texttt{\_build\_training\_step} --- creates a training step dict with cluster assignments
          and sizes.
    \item \texttt{\_build\_radiation\_step} --- creates a radiation step dict from
          \texttt{evaluate\_clustering} results and lobe metrics (Cm, CF, SLL, HPBW, cuts, etc.).
    \item \texttt{save\_checkpoint} --- saves a full checkpoint (notebook name, case name,
          description, timestamp, steps) to a JSON file.
\end{itemize}

These checkpoints enable cross-notebook comparison (e.g.\ GNN vs MC/GA results).

% ------------------------------------------------------------
\subsection{Cell 2 --- Section Header (Markdown)}
% ------------------------------------------------------------
Markdown: ``\textbf{1.\ Configuration}''. No code.

% ------------------------------------------------------------
\subsection{Cell 3 --- Array Configuration}
% ------------------------------------------------------------

\textbf{What it does.}
Creates a \texttt{URAConfig} object with the physical parameters of the antenna array:
\begin{itemize}
    \item \texttt{rows=16, cols=16} $\Rightarrow$ $N = 256$ elements.
    \item \texttt{dx=0.5} $\Rightarrow$ horizontal spacing = $0.5\lambda$ ($\approx 5.08\;\text{mm}$).
    \item \texttt{dy=0.7} $\Rightarrow$ vertical spacing = $0.7\lambda$ ($\approx 7.12\;\text{mm}$).
    \item \texttt{freq\_ghz=29.5} $\Rightarrow$ $f = 29.5\;\text{GHz}$, hence
          $\lambda = 3 \times 10^8 / 29.5 \times 10^9 \approx 1.017\;\text{cm}$.
\end{itemize}
Internally, \texttt{URAConfig} also computes \texttt{dx\_meters} and \texttt{dy\_meters}
(physical spacing in meters).

\textbf{Output.}
\begin{verbatim}
Array: 256 elements
Spacing: dx=0.5 lambda, dy=0.7 lambda
\end{verbatim}

% ------------------------------------------------------------
\subsection{Cell 4 --- Section Header (Markdown)}
% ------------------------------------------------------------
Markdown: ``\textbf{2.\ Train GNN (Physics-Informed)} --- Uses mutual coupling and targets a
clustering factor of $\sim$2.''

% ------------------------------------------------------------
\subsection{Cell 5 --- Train GNN (Physics-Informed)}
\label{sec:cell5}
% ------------------------------------------------------------

\textbf{What it does.}
Trains the GNN to partition 256 antennas into clusters using the physics-informed pipeline
with a target clustering factor of $\text{CF} = 2$ for 2000 epochs.
The number of clusters $K$ is \emph{not} fixed---the GNN discovers it automatically.

\paragraph{Inside \texttt{train\_ura\_clustering}:}
\begin{enumerate}
    \item \textbf{Upper bound on clusters.}
          $K_{\max} = \min\bigl(N,\; \lfloor N / \text{CF}_{\text{target}} \times 1.5 \rfloor\bigr)$.
          For $\text{CF}=2$: $K_{\max} = \min(256, 192) = 192$.
          The GNN outputs $K_{\max}$ soft clusters and deactivates unused ones during training.

    \item \textbf{Graph construction} (\texttt{create\_ura\_graph} with \texttt{use\_coupling=True}).
    \begin{itemize}
        \item Each of the 256 antennas becomes a \emph{node}.
              Its feature vector is its 2D position $(x, y)$ in wavelengths.
        \item Edges connect each element to its 8 grid neighbors (8-connected).
        \item The mutual coupling matrix $\mathbf{M} \in \mathbb{C}^{256 \times 256}$ is computed:
              $M_{ij} = \frac{e^{-j2\pi r_{ij}}}{r_{ij} + 0.1} \times f_{\text{edge}}$
              where $r_{ij}$ is the distance in wavelengths and $f_{\text{edge}}$ is a correction
              factor (0.85$\times$ for edge elements, additional 0.9$\times$ for corners).
        \item Each edge carries a 4-component feature vector $[\Delta x, \Delta y, d, |M_{ij}|]$
              (spatial offsets, distance, coupling magnitude).
        \item The coupling-weighted adjacency $\mathbf{W} = |\mathbf{M}|$ (full $N \times N$ matrix)
              is used in the MinCut loss.
    \end{itemize}

    \item \textbf{Position normalization.}
          All $(x,y)$ coordinates are min-max normalized to $[0, 1]$.

    \item \textbf{Model instantiation} (\texttt{URAClusteringGNN}).
    \begin{itemize}
        \item Architecture: \texttt{Linear(2$\to$64)} $\to$ \texttt{BatchNorm} $\to$ \texttt{ELU}
              $\to$ \texttt{GATConv}$_1$(64$\to$256, 4 heads) $\to$ \texttt{BatchNorm} $\to$ \texttt{ELU}
              $\to$ \texttt{GATConv}$_2$(256$\to$256, 4 heads) $\to$ \texttt{BatchNorm} $\to$ \texttt{ELU}
              $\to$ \texttt{GATConv}$_3$(256$\to$64, 1 head) $\to$ \texttt{ELU}
              $\to$ \texttt{Linear(64$\to K_{\max}$)} $\to$ \texttt{Gumbel-Softmax}.
        \item \texttt{GATConv} = Graph Attention Convolution. Each node aggregates features
              from its neighbors using learned attention weights.
              The attention mechanism lets the network learn which neighbors matter more.
        \item The classifier layer is initialised with Xavier uniform (gain $= 0.5$) to
              produce diverse initial logits and break symmetry.
        \item During \textbf{training}: the logits are passed through
              \texttt{F.gumbel\_softmax(logits, tau=$T$)} which adds Gumbel noise for
              exploration and uses the temperature $T$ to control assignment sharpness.
        \item During \textbf{evaluation}: standard \texttt{F.softmax(logits/$T$)} is used
              for deterministic assignments.
        \item The final output is $\mathbf{Z} \in \mathbb{R}^{256 \times K_{\max}}$:
              a soft cluster-assignment matrix where $Z_{ik}$ is the probability that
              antenna $i$ belongs to cluster $k$.
    \end{itemize}

    \item \textbf{Temperature annealing.}
    The Gumbel-Softmax temperature $T$ is linearly annealed:
    \[
        T(\text{epoch}) = \begin{cases}
            5.0 - \dfrac{4.5 \cdot \text{epoch}}{0.7 \cdot E} & \text{if epoch} < 0.7E \\[6pt]
            0.5 & \text{otherwise}
        \end{cases}
    \]
    where $E$ is the total number of epochs.
    High temperature ($T=5$) at the start produces soft, exploratory assignments with
    non-zero gradients.
    Low temperature ($T=0.5$) at the end produces near-hard, decisive assignments.
    This is critical because with uniform softmax ($T=1$, no noise), the MinCut loss
    gradient is exactly zero at the uniform fixed point $Z_{ik} = 1/K$,
    preventing any learning.

    \item \textbf{Loss function} (what we minimize). Five terms (six when
          \texttt{allowed\_sizes} is specified):
    \begin{enumerate}[label=\alph*)]
        \item \textbf{Coupling MinCut loss} (weight 1.0):
              \[
                  \mathcal{L}_{\text{cut}} = -\frac{\text{tr}(\mathbf{Z}^\top \mathbf{W}\, \mathbf{Z})}
                  {\text{tr}(\mathbf{Z}^\top \mathbf{D}\, \mathbf{Z}) + \epsilon}
              \]
              where $\mathbf{W} = |\mathbf{M}|$ is the coupling-weighted adjacency matrix,
              $\mathbf{D} = \text{diag}(\mathbf{W}\,\mathbf{1})$, and $\epsilon = 10^{-8}$.
              Minimizing this encourages the network to place \emph{strongly coupled} elements
              in the \emph{same} cluster.

        \item \textbf{Clustering factor loss} (weight $\lambda_{\text{cf}} = 10.0$):
              \[
                  \mathcal{L}_{\text{cf}} = \left(\frac{n_{\text{active}} - N / \text{CF}_{\text{target}}}
                  {N / \text{CF}_{\text{target}}}\right)^2
              \]
              where $n_{\text{active}} = \sum_{k=1}^{K_{\max}} \sigma\bigl(5 \cdot (s_k - 0.5)\bigr)$
              is the differentiable (sigmoid-based) count of active clusters,
              and $s_k = \sum_{i} Z_{ik}$ is the soft size of cluster $k$.
              This steers the GNN toward the desired average cluster size without
              fixing the number of clusters \emph{a priori}.

        \item \textbf{Balance loss} (weight $\lambda_{\text{bal}} = 5.0$):
              \[
                  \mathcal{L}_{\text{bal}} = \frac{1}{|\mathcal{A}|}
                  \sum_{k \in \mathcal{A}} \frac{(s_k - \text{CF}_{\text{target}})^2}
                  {\text{CF}_{\text{target}}^2}
              \]
              where $\mathcal{A} = \{k : s_k > 0.5\}$ is the set of active clusters.
              Penalizes imbalanced cluster sizes: each active cluster should have
              approximately $\text{CF}_{\text{target}}$ elements.

        \item \textbf{Entropy loss} (weight $\lambda_{\text{ent}} = 0.5$):
              \[
                  \mathcal{L}_{\text{ent}} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K}
                  Z_{ik} \log(Z_{ik} + \epsilon)
              \]
              Encourages \emph{confident} assignments (low entropy $\Rightarrow$ each node is
              assigned to one cluster with high probability, not spread evenly).

        \item \textbf{Contiguity loss} (weight $\lambda_{\text{cont}} = 0.5$):
              \[
                  \mathcal{L}_{\text{cont}} = \frac{1}{K}\sum_{k=1}^{K}
                  \frac{\sum_{i} Z_{ik} \|\mathbf{p}_i - \boldsymbol{\mu}_k\|^2}{s_k}
              \]
              where $\boldsymbol{\mu}_k = \sum_i Z_{ik}\,\mathbf{p}_i \,/\, s_k$ is the soft
              centroid of cluster $k$. Penalizes fragmented clusters by encouraging spatial
              compactness.

        \item \textbf{Allowed-sizes loss} (weight $\lambda_{\text{as}} = 5.0$, optional):
              \[
                  \mathcal{L}_{\text{as}} = \frac{1}{|\mathcal{A}|}
                  \sum_{k \in \mathcal{A}} \min_{a \in \mathcal{S}} (s_k - a)^2
              \]
              where $\mathcal{S}$ is the set of allowed sizes (e.g.\ $\{1, 2, 4\}$).
              Each active cluster's soft size is penalized by its squared distance to the
              nearest allowed size. Only active when \texttt{allowed\_sizes} is specified.
    \end{enumerate}

    Total loss:
    \[
        \mathcal{L} = \mathcal{L}_{\text{cut}}
        + 10 \cdot \mathcal{L}_{\text{cf}}
        + 5 \cdot \mathcal{L}_{\text{bal}}
        + 0.5 \cdot \mathcal{L}_{\text{ent}}
        + 0.5 \cdot \mathcal{L}_{\text{cont}}
        \;[\;+ 5 \cdot \mathcal{L}_{\text{as}}\;]
    \]

    \item \textbf{Optimizer:} AdamW (lr $= 0.001$, weight decay $= 10^{-4}$) with cosine
          annealing learning rate schedule, for 2000 epochs.
          Gradient clipping with max norm $1.0$ for stability.

    \item \textbf{NaN recovery:} if NaN values are detected during training, the model is
          reinitialised and training continues from the current epoch.

    \item \textbf{Best checkpoint:} the cluster assignment with the lowest total loss
          seen during training is saved. If the final result is degenerate (single cluster),
          the best checkpoint is used instead.

    \item \textbf{Post-processing (when \texttt{allowed\_sizes} is set):}
          After extracting hard assignments, the function \texttt{\_snap\_to\_allowed\_sizes}
          greedily splits any cluster whose size is not in the allowed set.
          For example, a cluster of size 3 is split into $2+1$; size 5 into $4+1$;
          size 6 into $4+2$; size 7 into $4+2+1$; size 8 into $4+4$.
          The splitting uses the largest allowed size first.

    \item \textbf{Output:} After training, hard assignments are extracted:
          $\text{cluster}(i) = \arg\max_k Z_{ik}$.
          Cluster labels are relabelled to consecutive integers (gaps from empty clusters
          are removed).
\end{enumerate}

\textbf{Output.}
The training prints progress every 100 epochs, showing the total loss, temperature $T$,
number of active clusters, current clustering factor, cluster size statistics (min, max, mean),
and individual loss components (cut, cf, balance).
The final output reports the discovered number of clusters and the achieved CF.

% ------------------------------------------------------------
\subsection{Cell 6 --- Section Header (Markdown)}
% ------------------------------------------------------------
Markdown: ``\textbf{3.\ Visualize Results}''.

% ------------------------------------------------------------
\subsection{Cell 7 --- Visualization}
% ------------------------------------------------------------

\textbf{What it does.}
Creates two side-by-side plots:

\begin{enumerate}
    \item \textbf{Scatter plot} (left): each antenna element is a dot at its physical $(x,y)$
          position, colored by cluster assignment.
          The colormap adapts to the number of active clusters discovered by the GNN
          (\texttt{nipy\_spectral} for $>20$, \texttt{tab20} for $>10$, \texttt{tab10} otherwise).

    \item \textbf{Grid view} (right): the cluster assignments reshaped into the $16 \times 16$ grid,
          displayed as a color-coded image.
          This is the ``bird's-eye view'' of which antenna belongs to which cluster.
\end{enumerate}

% ------------------------------------------------------------
\subsection{Cell 11 --- Section Header (Markdown)}
% ------------------------------------------------------------
Markdown: ``\textbf{5.\ Radiation Pattern (if available)}''.

% ------------------------------------------------------------
\subsection{Cell 12 --- Radiation Pattern Evaluation}
\label{sec:cell12}
% ------------------------------------------------------------

This is the largest and most important evaluation cell.
It defines helper functions and then computes the far-field radiation pattern
for the GNN clustering result.

\subsubsection{Helper function: \texttt{extract\_lobe\_metrics}}

\textbf{Input:} 2D far-field pattern $\text{FF\_I\_dB}[\theta, \phi]$ in dBi, the
azimuth/elevation angle vectors, the beam-steering direction $(\phi_0, \theta_0)$,
and the boresight gain.

\textbf{What it computes:}
\begin{itemize}
    \item Extracts the elevation cut (FF at fixed $\phi = \phi_0$) and azimuth cut
          (FF at fixed $\theta = \theta_0$).
    \item \textbf{HPBW} (Half-Power Beamwidth): finds the $-3\;\text{dB}$ crossing points
          around the main lobe peak in both cuts.
          Narrower HPBW means a more focused beam.
    \item \textbf{SLL} (Side-Lobe Level): identifies all local maxima (via \texttt{scipy.find\_peaks})
          that are $>3\;\text{dB}$ below the main lobe, and reports the highest one.
          Lower SLL means less power wasted in unwanted directions.
    \item \textbf{Number of lobes}: counts all peaks above $-30\;\text{dB}$.
\end{itemize}

\textbf{Output:} a dictionary with gain, HPBW, SLL, and lobe counts.

\subsubsection{Helper function: \texttt{plot\_lobe\_analysis}}

\textbf{Input:} the same pattern data plus the \texttt{AntennaArray} object.

\textbf{What it plots} (6-panel figure):
\begin{enumerate}
    \item Elevation cut (Cartesian) with $-3\;\text{dB}$ line and SLL marker.
    \item Azimuth cut (Cartesian) with $-3\;\text{dB}$ line and SLL marker.
    \item 2D contour map of the full far-field pattern (azimuth vs.\ elevation), with
          a star at the steering direction.
    \item Summary table of all metrics.
    \item Elevation pattern in polar coordinates.
    \item Azimuth pattern in polar coordinates.
\end{enumerate}

\subsubsection{Radiation evaluation pipeline}

The cell performs the following steps:
\begin{enumerate}
    \item \textbf{Format conversion} (\texttt{assignments\_to\_antenna\_format}):
          converts the flat vector $[0, 2, 1, 3, \ldots]$ into a list of $K$ arrays,
          each containing the lattice coordinates $(\text{NN}, \text{MM})$ of elements
          in that cluster.
          These lattice coordinates match the indexing convention used by the
          antenna physics engine.
          Empty clusters (if any) are automatically skipped.

    \item \textbf{Antenna array setup}: creates an \texttt{AntennaArray} object with:
    \begin{itemize}
        \item \texttt{LatticeConfig}: $16 \times 16$, spacing $(0.7\lambda, 0.5\lambda)$.
        \item \texttt{SystemConfig}: $f = 29.5\;\text{GHz}$, beam steered to
              $(\theta_0, \phi_0) = (0^\circ, 0^\circ)$, angular resolution
              $\Delta\theta = \Delta\phi = 0.5^\circ$.
        \item \texttt{MaskConfig}: SLL mask thresholds---defines the acceptable
              side-lobe levels inside and outside the field of view.
        \item \texttt{ElementPatternConfig}: $P=1$ means the element pattern is
              $\cos(\theta)\cos(\phi)$, element gain $G_{\text{el}} = 5\;\text{dBi}$.
    \end{itemize}

    \item \textbf{Radiation pattern computation} (\texttt{evaluate\_clustering}):
    \begin{enumerate}[label=\roman*)]
        \item Converts cluster coordinates to physical positions $(Y_c, Z_c)$ in meters.
        \item Computes cluster centroids $(Y_{c,m}, Z_{c,m})$.
        \item Computes excitation coefficients:
              $c_0^{(k)} = \frac{1}{L_k} \exp\!\bigl(-j(w_0 Z_{c,m}^{(k)} + v_0 Y_{c,m}^{(k)})\bigr)$
              where $w_0, v_0$ encode the steering direction and $L_k$ is the cluster size.
              Each cluster gets a single complex weight (amplitude $1/L_k$, phase for steering).
        \item Computes the array factor via the kernel:
              $\text{KerFF}(w,v) = \sum_{\text{elements}} F_{\text{el}}(w,v) \cdot e^{j(v \cdot Y + w \cdot Z)}$
              summed per cluster, then combined as $\text{FF} = \text{KerFF} \cdot c_0^\top$.
        \item Interpolates from $(w,v)$ spectral domain to $(\theta, \phi)$ angular domain.
        \item Adds element pattern gain: $\text{FF\_I\_dB} = \text{FF\_norm\_dB} + G_{\text{el,max}} + 10\log_{10}(N_{\text{active}})$.
        \item Computes $C_m$: the mean of the top-10 worst SLL mask violations (in dB).
              $C_m = 0$ means no violation.
        \item Computes SLL in-FoV and out-of-FoV (excluding a $\pm10^\circ$ main-lobe exclusion zone).
    \end{enumerate}

    \item \textbf{Plots} the 6-panel lobe analysis figure.

    \item \textbf{Prints} the number of clusters, clustering factor, $C_m$,
          SLL out-of-FoV, SLL in-FoV, and \textbf{hardware reduction}
          $= (1 - K/N) \times 100\%$.
\end{enumerate}

% ------------------------------------------------------------
\subsection{Initial Checkpoint Save Cell}
% ------------------------------------------------------------

Saves the initial GNN result to \texttt{gnn\_initial.json} in the
\texttt{intermediate\_steps/} directory, including config, training, and radiation data.

\newpage
% ============================================================
\section{Test Cases}
% ============================================================

The remaining cells implement 6 test cases (Case~0 through Case~5) to evaluate the GNN
under different steering directions, target clustering factors, and SLL constraints.
All cases share:
\begin{itemize}
    \item $f = 29.5\;\text{GHz}$, element pattern $\cos(\theta)\cos(\phi)$ ($P=1$).
    \item $N = 256$ ($16 \times 16$), spacing $d_y = 0.7\lambda$, $d_x = 0.5\lambda$.
    \item Physics-informed GNN mode (2000 epochs), Gumbel-Softmax with temperature annealing,
          mutual coupling edge weights.
    \item The number of clusters $K$ is auto-discovered to achieve the target CF.
    \item \textbf{Hardware reduction} printed: $(1 - K/N) \times 100\%$.
    \item Checkpoint saved to JSON after each case.
\end{itemize}

Each test case follows the \textbf{same pipeline}:
\begin{enumerate}
    \item Create \texttt{URAConfig} $\to$ train GNN with \texttt{target\_cf}
          (and optionally \texttt{allowed\_sizes}) $\to$ get cluster assignments.
    \item Visualize clusters (scatter + grid).
    \item Create \texttt{AntennaArray} with case-specific steering and mask.
    \item Convert assignments $\to$ evaluate radiation $\to$ plot $\to$ print results
          (including hardware reduction).
    \item Save checkpoint to JSON.
\end{enumerate}

% ------------------------------------------------------------
\subsection{Case 0: $\theta=0^\circ$, $\phi=0^\circ$, target CF$=2$, SLL $< -20\;\text{dB}$, sizes $\{1,2,4\}$}
% ------------------------------------------------------------

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Beam steering & boresight $\theta_0 = 0^\circ$, $\phi_0 = 0^\circ$ \\
Target clustering factor & $\text{CF} = 2$ ($\approx 2$ elements/cluster) \\
SLL requirement & $< -20\;\text{dB}$ (both in-FoV and out-of-FoV) \\
Allowed cluster sizes & $\{1, 2, 4\}$ (enforced via loss + post-processing) \\
FoV mask & elevation $\pm30^\circ$, azimuth $\pm60^\circ$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{What happens.}
This is the baseline case: no beam steering (boresight), moderate compression ($\text{CF}=2$),
and cluster sizes restricted to $\{1, 2, 4\}$.
The allowed-sizes constraint is enforced in two ways:
\begin{enumerate}
    \item \textbf{During training}: the allowed-sizes loss $\mathcal{L}_{\text{as}}$
          penalizes cluster sizes that are not in $\{1, 2, 4\}$ (soft constraint).
    \item \textbf{After training}: the \texttt{\_snap\_to\_allowed\_sizes} function
          greedily splits any remaining invalid-size clusters (hard constraint).
\end{enumerate}

After printing the GNN results, the cell loads the MC/GA Case~0 checkpoint
(\texttt{clustering\_case\_0.json}) and prints a \textbf{comparison table} showing
hardware reduction and performance differences (Cm, SLL\_out, SLL\_in) between
GNN and Monte Carlo / Genetic Algorithm.

% ------------------------------------------------------------
\subsection{Case 1: $\theta=10^\circ$, $\phi=0^\circ$, target CF$=2$, SLL $< -20\;\text{dB}$}
% ------------------------------------------------------------

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Beam steering & elevation $\theta_0 = 10^\circ$, azimuth $\phi_0 = 0^\circ$ \\
Target clustering factor & $\text{CF} = 2$ ($\approx 2$ elements/cluster) \\
SLL requirement & $< -20\;\text{dB}$ (both in-FoV and out-of-FoV) \\
FoV mask & elevation $\pm30^\circ$, azimuth $\pm60^\circ$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{What happens.}
With $\text{CF} = 2$, the GNN targets $\approx 128$ clusters of $\approx 2$ elements each.
This is close to the no-clustering case (each element independent), so
radiation performance should be near-optimal.
The beam is steered $10^\circ$ off boresight in elevation.

% ------------------------------------------------------------
\subsection{Case 2: $\theta=0^\circ$, $\phi=60^\circ$, target CF$=2$, SLL $< -20\;\text{dB}$}
% ------------------------------------------------------------

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Beam steering & elevation $\theta_0 = 0^\circ$, azimuth $\phi_0 = 60^\circ$ \\
Target clustering factor & $\text{CF} = 2$ \\
SLL requirement & $< -20\;\text{dB}$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{What happens.}
The beam is steered to $60^\circ$ in azimuth---an extreme angle near the edge of the
field of view. This is the hardest steering case because the array factor degrades
at wide scan angles and grating lobes may appear (especially with $d_x = 0.5\lambda$).

% ------------------------------------------------------------
\subsection{Case 3: $\theta=10^\circ$, $\phi=45^\circ$, target CF$=4$, SLL $< -20\;\text{dB}$}
% ------------------------------------------------------------

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Beam steering & elevation $\theta_0 = 10^\circ$, azimuth $\phi_0 = 45^\circ$ \\
Target clustering factor & $\text{CF} = 4$ ($\approx 4$ elements/cluster) \\
SLL requirement & $< -20\;\text{dB}$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{What happens.}
Diagonal steering---the beam points simultaneously off-axis in both elevation and azimuth.
With $\text{CF} = 4$, the GNN targets $\approx 64$ clusters.
This combines the challenges of Cases 1 and 2 with higher compression.

% ------------------------------------------------------------
\subsection{Case 4: $\theta=10^\circ$, $\phi=45^\circ$, target CF$=4$, SLL $< -15\;\text{dB}$}
% ------------------------------------------------------------

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Beam steering & elevation $\theta_0 = 10^\circ$, azimuth $\phi_0 = 45^\circ$ \\
Target clustering factor & $\text{CF} = 4$ ($\approx 4$ elements/cluster) \\
SLL requirement & $< -15\;\text{dB}$ (relaxed) \\
\bottomrule
\end{tabular}
\end{center}

\textbf{What happens.}
Same steering and CF as Case~3, but with a relaxed SLL constraint ($-15\;\text{dB}$
instead of $-20\;\text{dB}$).
This tests whether the GNN can produce a useful clustering with higher compression
and relaxed requirements.

% ------------------------------------------------------------
\subsection{Case 5: $\theta=0^\circ$, $\phi=0^\circ$, target CF$=8$}
% ------------------------------------------------------------

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Beam steering & boresight $\theta_0 = 0^\circ$, $\phi_0 = 0^\circ$ \\
Target clustering factor & $\text{CF} = 8$ ($\approx 8$ elements/cluster) \\
SLL requirement & $-20\;\text{dB}$ out-of-FoV, $-15\;\text{dB}$ in-FoV \\
\bottomrule
\end{tabular}
\end{center}

\textbf{What happens.}
The most aggressive compression: with $\text{CF} = 8$, the GNN targets $\approx 32$ clusters
of $\approx 8$ elements each.
Steering is at boresight (easiest case), so we can afford the compression.
This represents the maximum hardware cost reduction ($\sim32\times$ fewer RF chains
compared to element-level control).

\newpage
% ============================================================
\section{Summary}
% ============================================================

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{p{2.5cm} p{10.5cm}}
\toprule
\textbf{Input} & A $16 \times 16$ antenna array at $29.5\;\text{GHz}$ with specified
element spacing, beam steering direction, target clustering factor $\text{CF}_{\text{target}}$,
SLL mask, and optionally a set of allowed cluster sizes. \\
\midrule
\textbf{What we optimize} & The assignment of 256 antenna elements into $K$ clusters
(where $K$ is auto-discovered), such that the average cluster size matches the target
clustering factor, clusters are spatially contiguous, cluster sizes belong to the allowed
set (if specified), and the resulting far-field radiation pattern meets the SLL requirements. \\
\midrule
\textbf{How} & A 3-layer Graph Attention Network (GAT) with mutual coupling edge weights
outputs soft cluster assignments $\mathbf{Z}$ via Gumbel-Softmax with temperature annealing
($T: 5.0 \to 0.5$), trained unsupervised by minimizing a composite loss:
coupling MinCut (group strongly-coupled elements),
clustering factor (target average cluster size),
balance (penalize imbalanced sizes),
entropy (confident assignments),
contiguity (spatial compactness),
and optionally allowed-sizes (restrict to valid sizes).
Post-processing snaps cluster sizes to allowed values when specified. \\
\midrule
\textbf{Output} & A cluster assignment vector $\mathbf{c} \in \{0,\dots,K{-}1\}^{256}$
where $K$ is automatically discovered,
the resulting far-field radiation pattern with metrics:
boresight gain, HPBW, SLL (in/out of FoV), cost function $C_m$,
and hardware reduction $(1 - K/N) \times 100\%$.
Checkpoints saved to JSON for cross-notebook comparison with MC/GA results. \\
\midrule
\textbf{Key result} & The physics-informed GNN discovers the optimal number of clusters
to achieve the target clustering factor while respecting mutual coupling structure
and spatial contiguity. Case~0 includes a direct comparison with Monte Carlo and
Genetic Algorithm baselines from \texttt{clustering\_comparison.ipynb}. \\
\bottomrule
\end{tabular}
\end{center}

\end{document}
